{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traffic_red_convolucional_gpu_bel",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbqHfvwZHnk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instalamos KERAS\n",
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODuccLWZZxJE",
        "colab_type": "code",
        "outputId": "a6c48f8c-c2f5-4d8b-e33f-09beffdf1eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "# Descargamos el dataset\n",
        "!git clone https://github.com/agonzalezhidalgo/dataset_bel.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dataset_bel'...\n",
            "remote: Enumerating objects: 7345, done.\u001b[K\n",
            "remote: Counting objects: 100% (7345/7345), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7334/7334), done.\u001b[K\n",
            "remote: Total 7345 (delta 2), reused 7342 (delta 2), pack-reused 0\n",
            "Receiving objects: 100% (7345/7345), 245.49 MiB | 14.81 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Checking out files: 100% (7223/7223), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDKgXudXaYJF",
        "colab_type": "code",
        "outputId": "970b10c8-45db-4600-8b86-de3f13ef430a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# http://scikit-image.org/docs/stable/api/api.html\n",
        "import skimage\n",
        "\n",
        "# https://docs.python.org/3/library/csv.html\n",
        "import csv\n",
        "\n",
        "# https://matplotlib.org/api/index.html\n",
        "import matplotlib\n",
        "\n",
        "# https://matplotlib.org/api/_as_gen/matplotlib.pyplot.html#module-matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# https://docs.python.org/3/library/os.html\n",
        "import os\n",
        "\n",
        "# https://docs.scipy.org/doc/numpy/reference/\n",
        "import numpy as np\n",
        "\n",
        "# https://keras.io/models/model/\n",
        "import keras\n",
        "\n",
        "from keras import models\n",
        "\n",
        "# Core Layers: https://keras.io/layers/core/\n",
        "# Convolution Layers: https://keras.io/layers/convolutional/\n",
        "from keras import layers\n",
        "\n",
        "# https://keras.io/preprocessing/image/\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# https://keras.io/callbacks/\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16_v-R69B9XO",
        "colab_type": "text"
      },
      "source": [
        "Celda con funciones generales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jP7IhbsUOsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Devuelve las imagenes y etiquetas de una carpeta especifíca\n",
        "def __read_images_labels_from_dir(directory, images, labels, shape, as_gray, get_label_from_dir):\n",
        "    \n",
        "    #Buscamos fotos en el directorio\n",
        "    for f in os.listdir(directory):\n",
        "        # Cargamos archivos con extension .ppm \n",
        "        # Obviamos el color y lo cargamos como escala de grises y normalizamos el tamaño\n",
        "        if f.endswith(\".ppm\"):\n",
        "            image = load_image(os.path.join(directory, f), shape, as_gray)\n",
        "            images.append(image)\n",
        "            if get_label_from_dir:\n",
        "                labels.append(int(os.path.basename(directory)))\n",
        "    \n",
        "    return images, labels\n",
        "  \n",
        "# Obtenemos el índice inicial y final de una label específica\n",
        "# - label: índice de la label que se quiere buscar.\n",
        "# - source: lista de la relación de etiquetas.\n",
        "\n",
        "def __get_start_end(label, source):\n",
        "\n",
        "    start = 0\n",
        "    end = 0\n",
        "    \n",
        "    try:        \n",
        "        start = source.index(label)\n",
        "        end = start + source.count(label)\n",
        "\n",
        "    except:\n",
        "        print(\"label doesn't exist\")\n",
        "\n",
        "    return start, end\n",
        "  \n",
        "# Devuelve una colección de directorios de la ruta\n",
        "def __get_directories(data_dir):\n",
        "    \n",
        "    directories = []\n",
        "    \n",
        "    if os.path.exists(data_dir):\n",
        "        \n",
        "        #Buscamos todos los directorios de la ruta\n",
        "        for d in os.listdir(data_dir):\n",
        "            if os.path.isdir(os.path.join(data_dir, d)):\n",
        "                directories.append(d)\n",
        "                \n",
        "    else:\n",
        "        print(\"path doesn't exists\")\n",
        "    return directories\n",
        "  \n",
        "# Devuelve una colección con las imágenes y los labels de la ruta\n",
        "# -datadir: path donde se encuentran la colección de imágenes.\n",
        "# -shape: Dimensiones con las que se cargarán las imágenes.\n",
        "# -as_gray: Indica si la imagen se cargará en escala de grises.\n",
        "# -get_label_from_dir: (defecto True) específica si la categoría se lee del propio directorio\n",
        "def readDataset(data_dir, shape, as_gray, get_label_from_dir = True):\n",
        "    images = []\n",
        "    labels = []\n",
        "    directories = __get_directories(data_dir)\n",
        "    \n",
        "    images, labels = __read_images_labels_from_dir(data_dir, images, labels, shape, as_gray, get_label_from_dir)\n",
        "    \n",
        "    for d in directories:\n",
        "                \n",
        "        #Buscamos fotos en el directorio\n",
        "        images_dir = os.path.join(data_dir, d)\n",
        "        images, labels = __read_images_labels_from_dir(images_dir, images, labels, shape, as_gray, get_label_from_dir)\n",
        "       \n",
        "    return images, labels\n",
        "  \n",
        "# Devuelve una imagen\n",
        "# - path: ruta de la imagen.\n",
        "# - size: dimensión con la que se cargará la imagen.\n",
        "# - as_gray: True para cargar la imagen en escala de grises.\n",
        "\n",
        "def load_image(path, size, as_gray):\n",
        "    aux = skimage.data.imread(path, as_gray = as_gray)\n",
        "    return skimage.transform.resize(aux, size, mode='constant')\n",
        "  \n",
        "# Devuelve una lista con los datos del fichero csv.\n",
        "# - path: ruta hasta el fichero csv.\n",
        "# - delimeter: Carácter delimitador de campos\n",
        "\n",
        "def read_csv(path, delimiter):\n",
        "    file = open(path)\n",
        "    title_csv = csv.reader(file, delimiter = delimiter)\n",
        "    return list(title_csv)\n",
        "  \n",
        "# Imprime los tamaños de las colecciones\n",
        "# - images: lista de imágenes precargadas.\n",
        "# - labels: relación de las imágenes con las categorías a las que pertenecen.\n",
        "# - np_images: np.array con las imágenes precargadas.\n",
        "# - np_labels: np.array con las categorías precargadas.\n",
        "# - environment: string identificativo del entorno.\n",
        "\n",
        "def print_size_dataset(images, labels, np_images, np_labels, environment):\n",
        "    print(\"Total images (\" + environment + \"): \", len(images))\n",
        "    print(\"Total labels (\" + environment + \"): \", len(set(labels)))\n",
        "    print(\"Images shape: \", np_images.shape)\n",
        "    print(\"Labels shape: \", np_labels.shape)\n",
        "    \n",
        "# Imprime una matriz 32x32 con los diferentes tipos de señales que\n",
        "# se van a clasificar. Muestra la primera imagen de cada categoría.\n",
        "# - images: lista de imágenes precargadas.\n",
        "# - labels: relación de las imágenes con las categorías a las que pertenecen.\n",
        "# - titles: lista con los nombres de las categorías.\n",
        "\n",
        "def print_summary_dataset(images, labels, titles):\n",
        "    \n",
        "    #Quitamos los repetidos a los labels\n",
        "    unique_labels = set(labels)\n",
        "    \n",
        "    plt.figure(figsize=(32, 32))\n",
        "    i = 1\n",
        "    \n",
        "    for label in unique_labels:\n",
        "        \n",
        "        #Obtenemos la primera imagén de cada label\n",
        "        image = images[labels.index(label)]\n",
        "        plt.subplot(8, 8, i)\n",
        "        plt.axis('off')\n",
        "        plt.title(titles[i-1][1])\n",
        "        i += 1\n",
        "        _ = plt.imshow(image)\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "# Imprime todas las imágenes de una label especifica\n",
        "# - label: índice de la label que se quiere buscar.\n",
        "# - images: lista de imágenes precargadas.\n",
        "# - source: lista de la relación de etiquetas.\n",
        "# - titles: lista con las diferentes categorías de labels.\n",
        "\n",
        "def print_signals(label, images, source, titles):\n",
        "    \n",
        "    start, end = __get_start_end(label, source)\n",
        "    \n",
        "    if start < end:\n",
        "        rows = (end - start) / 8\n",
        "\n",
        "        print(\"Signal: \", titles[label][1])\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        i = 1\n",
        "\n",
        "        for image in images[start:end]:\n",
        "            plt.subplot(rows + 1, 8, i)\n",
        "            plt.axis('off')\n",
        "            i += 1\n",
        "            plt.imshow(image)\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "# Imprime los atributos de las imágenes de una label especifica.\n",
        "# - label: índice de la label que se quiere buscar.\n",
        "# - images: lista de imágenes precargadas.\n",
        "# - source: lista de la relación de etiquetas.\n",
        "# - titles: lista con las diferentes categorías de labels.\n",
        "\n",
        "def print_signals_attributes(label, images, source, titles):\n",
        "    \n",
        "    start, end = __get_start_end(label, source)\n",
        "    \n",
        "    if start < end:\n",
        "        print(\"Signal: \", titles[label][1])\n",
        "        for image in images[start:end]:\n",
        "            print(\"shape: \", image.shape, \"\\tmin:\", image.min(), \"\\tmax: \", image.max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WZtFcFKbB6-",
        "colab_type": "code",
        "outputId": "f0f6723c-41e7-4320-e623-93ca79c72885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "# Mostramos información de CPU & GPU\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13960429195915122201\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 16547436714670873232\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 18082746678274940940\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14800692839\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 6516404354368309474\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpZFFb5OZ3Dg",
        "colab_type": "code",
        "outputId": "44e979ea-0ae4-485d-f522-a8fd7598a1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Obtenemos el directorio actual como trabajo.\n",
        "ROOT_PATH = os.getcwd()\n",
        "\n",
        "# Establecemos la dimensión de las imágenes.\n",
        "IMG_SHAPE = (64, 64)\n",
        "print(\"Tamaño de las imágenes de entrada: \", IMG_SHAPE)\n",
        "IMG_SHAPE_LEN = IMG_SHAPE[0] * IMG_SHAPE[1]\n",
        "print(\"Vectorizando la entrada, sería de un tamaño: \", IMG_SHAPE_LEN)\n",
        "\n",
        "# Obtenemos los paths de trabajo\n",
        "labels_path = os.path.join(ROOT_PATH, \"dataset_bel/labels.csv\")\n",
        "train_path = os.path.join(ROOT_PATH, \"dataset_bel/train\")\n",
        "test_path = os.path.join(ROOT_PATH, \"dataset_bel/test\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamaño de las imágenes de entrada:  (64, 64)\n",
            "Vectorizando la entrada, sería de un tamaño:  4096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_55nevcUQHR",
        "colab_type": "code",
        "outputId": "f04462c5-191f-434d-9bc8-b5351d8d8e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Cargamos las imágenes de entrenamiento y de test.\n",
        "images_train, labels_train = readDataset(train_path, IMG_SHAPE, False)\n",
        "images_test, labels_test = readDataset(test_path, IMG_SHAPE, False)\n",
        "\n",
        "# Convertimos las listas a array numpy de float32\n",
        "np_images_train = np.asarray(images_train, dtype = np.float32)\n",
        "np_labels_train = np.asarray(labels_train, dtype = np.int8)\n",
        "\n",
        "np_images_test = np.asarray(images_test, dtype = np.float32)\n",
        "np_labels_test = np.asarray(labels_test, dtype = np.int8)\n",
        "\n",
        "# Recuperamos los nombres de las categorias. Los diferentes tipo de señales\n",
        "# que se van a clasificar.\n",
        "titles = read_csv(labels_path, \",\")\n",
        "\n",
        "# Se imprime información de los datos cargados.\n",
        "print_size_dataset(images_train, labels_train, np_images_train, np_labels_train, \"train\")\n",
        "print_size_dataset(images_test, labels_test, np_images_test, np_labels_test, \"test\")\n",
        "print(\"Titles total: \", len(titles))\n",
        "print_signals_attributes(10, images_train, labels_train, titles)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total images (train):  4575\n",
            "Total labels (train):  62\n",
            "Images shape:  (4575, 64, 64, 3)\n",
            "Labels shape:  (4575,)\n",
            "Total images (test):  2520\n",
            "Total labels (test):  53\n",
            "Images shape:  (2520, 64, 64, 3)\n",
            "Labels shape:  (2520,)\n",
            "Titles total:  62\n",
            "Signal:  A31\n",
            "shape:  (64, 64, 3) \tmin: 0.0064950980392159095 \tmax:  0.996078431372549\n",
            "shape:  (64, 64, 3) \tmin: 0.04844037224264742 \tmax:  0.9921568627450981\n",
            "shape:  (64, 64, 3) \tmin: 0.033517156862744875 \tmax:  0.8648241230085848\n",
            "shape:  (64, 64, 3) \tmin: 0.05310968137254885 \tmax:  0.571966911764706\n",
            "shape:  (64, 64, 3) \tmin: 0.024230238970588236 \tmax:  0.9842294730392157\n",
            "shape:  (64, 64, 3) \tmin: 0.0081806257659327 \tmax:  0.9777087641697311\n",
            "shape:  (64, 64, 3) \tmin: 0.07279411764705883 \tmax:  0.7749540441176471\n",
            "shape:  (64, 64, 3) \tmin: 0.012683823529412207 \tmax:  1.0\n",
            "shape:  (64, 64, 3) \tmin: 0.01371017156862504 \tmax:  0.9921568627450981\n",
            "shape:  (64, 64, 3) \tmin: 0.0 \tmax:  0.6835477941176472\n",
            "shape:  (64, 64, 3) \tmin: 0.06834022671568622 \tmax:  0.8909658394607853\n",
            "shape:  (64, 64, 3) \tmin: 0.022579656862743984 \tmax:  0.996078431372549\n",
            "shape:  (64, 64, 3) \tmin: 0.0391180300245094 \tmax:  0.8150242225796563\n",
            "shape:  (64, 64, 3) \tmin: 0.05793504901960768 \tmax:  0.9921568627450981\n",
            "shape:  (64, 64, 3) \tmin: 0.07595454197303977 \tmax:  0.7563754212622538\n",
            "shape:  (64, 64, 3) \tmin: 0.023467419194240183 \tmax:  0.7593352673100491\n",
            "shape:  (64, 64, 3) \tmin: 0.03903664981617604 \tmax:  0.9793401979932586\n",
            "shape:  (64, 64, 3) \tmin: 0.04133013556985346 \tmax:  1.0\n",
            "shape:  (64, 64, 3) \tmin: 0.03416077856924062 \tmax:  0.7854355755974258\n",
            "shape:  (64, 64, 3) \tmin: 0.03529411764705882 \tmax:  0.996078431372549\n",
            "shape:  (64, 64, 3) \tmin: 0.06338536879595565 \tmax:  0.9201834884344352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W2tKUgvUukC",
        "colab_type": "code",
        "outputId": "e464836c-77c4-4da8-8f3f-3a69f519b789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# Convertimos las labels de manera categórica\n",
        "labels_categorical_train = keras.utils.to_categorical(np_labels_train)\n",
        "labels_categorical_test = keras.utils.to_categorical(np_labels_test)\n",
        "\n",
        "print(\"Ejemplo primera imagen de manera categórica: \", labels_categorical_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ejemplo primera imagen de manera categórica:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhL5rFelZfLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_keras_model():\n",
        "    # IMPLEMENTACIÓN RED NEURONAL\n",
        "    # En Keras la envoltura para cualquier red neuronal se crea con la clase Sequential\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(32, (5, 5),\n",
        "                            activation='relu', input_shape=(IMG_SHAPE[0], IMG_SHAPE[1], 3)))\n",
        "    model.add(layers.MaxPooling2D(2, 2))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(2, 2))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    \n",
        "    model.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(2, 2))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    model.add(layers.MaxPooling2D(2, 2))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(len(set(labels_train)), activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Cucwmicr1K",
        "colab_type": "code",
        "outputId": "dc944546-13da-4c3c-e748-b8ad7158483b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1822
        }
      },
      "source": [
        "model = get_keras_model()\n",
        "\n",
        "# Muestra la arquitectura de nuestra red neuronal\n",
        "model.summary()\n",
        "\n",
        "# Configurando el modelo de aprendijaze:\n",
        "#  · loss, función para evaluar el grado de error entre salidas calculadas\n",
        "#  · optimizador, función para calcular los pesos de los parámetros a partir de los datos de entrada\n",
        "#  · metricas, para monitorizar el proceso de aprendizaje de la red.\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "             optimizer=\"sgd\",\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "# - batch_size, indica el número de datos que se usan en cada actualización.\n",
        "# - epochs, indica el número de veces que se usan todos los datos del proceso.\n",
        "#model.fit(np_images_train, labels_categorical_train, batch_size=32, epochs=20)\n",
        "model.fit(np_images_train, labels_categorical_train,\n",
        "          batch_size = 32,\n",
        "          epochs = 30,\n",
        "          callbacks=[ModelCheckpoint('model_64_64_bel_gpu.h5', save_best_only = False)])\n",
        "\n",
        "# Evaluación del modelo\n",
        "test_loss, test_acc = model.evaluate(np_images_test, labels_categorical_test)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 60, 60, 32)        2432      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 9, 9, 128)         204928    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 62)                31806     \n",
            "=================================================================\n",
            "Total params: 290,430\n",
            "Trainable params: 290,430\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/30\n",
            "4575/4575 [==============================] - 5s 1ms/step - loss: 3.6582 - acc: 0.1373\n",
            "Epoch 2/30\n",
            "4575/4575 [==============================] - 1s 254us/step - loss: 2.8938 - acc: 0.3292\n",
            "Epoch 3/30\n",
            "4575/4575 [==============================] - 1s 254us/step - loss: 2.2225 - acc: 0.4741\n",
            "Epoch 4/30\n",
            "4575/4575 [==============================] - 1s 252us/step - loss: 1.7032 - acc: 0.5893\n",
            "Epoch 5/30\n",
            "4575/4575 [==============================] - 1s 254us/step - loss: 1.3166 - acc: 0.6839\n",
            "Epoch 6/30\n",
            "4575/4575 [==============================] - 1s 253us/step - loss: 1.0450 - acc: 0.7366\n",
            "Epoch 7/30\n",
            "4575/4575 [==============================] - 1s 259us/step - loss: 0.8443 - acc: 0.7849\n",
            "Epoch 8/30\n",
            "4575/4575 [==============================] - 1s 252us/step - loss: 0.7084 - acc: 0.8216\n",
            "Epoch 9/30\n",
            "4575/4575 [==============================] - 1s 253us/step - loss: 0.5998 - acc: 0.8435\n",
            "Epoch 10/30\n",
            "4575/4575 [==============================] - 1s 269us/step - loss: 0.5198 - acc: 0.8706\n",
            "Epoch 11/30\n",
            "4575/4575 [==============================] - 1s 273us/step - loss: 0.4621 - acc: 0.8789\n",
            "Epoch 12/30\n",
            "4575/4575 [==============================] - 1s 272us/step - loss: 0.3909 - acc: 0.9005\n",
            "Epoch 13/30\n",
            "4575/4575 [==============================] - 1s 273us/step - loss: 0.3414 - acc: 0.9148\n",
            "Epoch 14/30\n",
            "4575/4575 [==============================] - 1s 274us/step - loss: 0.3107 - acc: 0.9174\n",
            "Epoch 15/30\n",
            "4575/4575 [==============================] - 1s 277us/step - loss: 0.2799 - acc: 0.9255\n",
            "Epoch 16/30\n",
            "4575/4575 [==============================] - 1s 273us/step - loss: 0.2595 - acc: 0.9303\n",
            "Epoch 17/30\n",
            "4575/4575 [==============================] - 1s 276us/step - loss: 0.2475 - acc: 0.9362\n",
            "Epoch 18/30\n",
            "4575/4575 [==============================] - 1s 258us/step - loss: 0.2422 - acc: 0.9384\n",
            "Epoch 19/30\n",
            "4575/4575 [==============================] - 1s 260us/step - loss: 0.2128 - acc: 0.9423\n",
            "Epoch 20/30\n",
            "4575/4575 [==============================] - 1s 255us/step - loss: 0.1866 - acc: 0.9508\n",
            "Epoch 21/30\n",
            "4575/4575 [==============================] - 1s 257us/step - loss: 0.1781 - acc: 0.9530\n",
            "Epoch 22/30\n",
            "4575/4575 [==============================] - 1s 255us/step - loss: 0.1530 - acc: 0.9585\n",
            "Epoch 23/30\n",
            "4575/4575 [==============================] - 1s 255us/step - loss: 0.1564 - acc: 0.9593\n",
            "Epoch 24/30\n",
            "4575/4575 [==============================] - 1s 260us/step - loss: 0.1484 - acc: 0.9591\n",
            "Epoch 25/30\n",
            "4575/4575 [==============================] - 1s 261us/step - loss: 0.1360 - acc: 0.9648\n",
            "Epoch 26/30\n",
            "4575/4575 [==============================] - 1s 256us/step - loss: 0.1295 - acc: 0.9644\n",
            "Epoch 27/30\n",
            "4575/4575 [==============================] - 1s 254us/step - loss: 0.1256 - acc: 0.9666\n",
            "Epoch 28/30\n",
            "4575/4575 [==============================] - 1s 254us/step - loss: 0.1171 - acc: 0.9685\n",
            "Epoch 29/30\n",
            "4575/4575 [==============================] - 1s 256us/step - loss: 0.1085 - acc: 0.9727\n",
            "Epoch 30/30\n",
            "4575/4575 [==============================] - 1s 259us/step - loss: 0.1034 - acc: 0.9753\n",
            "2520/2520 [==============================] - 0s 136us/step\n",
            "Test loss: 0.16938848919577024\n",
            "Test accuracy: 0.9595238095238096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgTG5A20csHI",
        "colab_type": "code",
        "outputId": "9948b8ae-d932-4c79-da16-947c8486e322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Evaluación del modelo\n",
        "test_loss, test_acc = model.evaluate(np_images_test, labels_categorical_test)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "y_pred = model.predict_classes(np_images_test)\n",
        "acc = np.sum(y_pred == np_labels_test) / np.size(np_labels_test)\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2520/2520 [==============================] - 0s 124us/step\n",
            "Test loss: 0.16938848919577024\n",
            "Test accuracy: 0.9595238095238096\n",
            "Test accuracy: 0.9595238095238096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgI3rrdScsRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(featurewise_center=False,\n",
        "                             featurewise_std_normalization=False, \n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=0.2,\n",
        "                             shear_range=0.1,\n",
        "                             rotation_range=10.,)\n",
        "\n",
        "datagen.fit(np_images_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QftMYJbmcz3d",
        "colab_type": "code",
        "outputId": "046be0ee-ffdf-4b45-e6d0-4d94510cdfe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "source": [
        "model_2 = get_keras_model()\n",
        "\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "             optimizer=\"sgd\",\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model_2.fit_generator(datagen.flow(np_images_train, labels_categorical_train, batch_size=32),\n",
        "                      steps_per_epoch = np_images_train.shape[0],\n",
        "                      epochs = 30,\n",
        "                      callbacks=[ModelCheckpoint('model_data_aug_64_64_bel_gpu.h5',save_best_only = False)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "4575/4575 [==============================] - 162s 35ms/step - loss: 1.0205 - acc: 0.7296\n",
            "Epoch 2/30\n",
            "4575/4575 [==============================] - 161s 35ms/step - loss: 0.2218 - acc: 0.9343\n",
            "Epoch 3/30\n",
            "4575/4575 [==============================] - 162s 35ms/step - loss: 0.1235 - acc: 0.9631\n",
            "Epoch 4/30\n",
            "4575/4575 [==============================] - 165s 36ms/step - loss: 0.0868 - acc: 0.9743\n",
            "Epoch 5/30\n",
            "4575/4575 [==============================] - 164s 36ms/step - loss: 0.0647 - acc: 0.9801\n",
            "Epoch 6/30\n",
            "4575/4575 [==============================] - 161s 35ms/step - loss: 0.0519 - acc: 0.9839\n",
            "Epoch 7/30\n",
            "4575/4575 [==============================] - 164s 36ms/step - loss: 0.0430 - acc: 0.9866\n",
            "Epoch 8/30\n",
            "4575/4575 [==============================] - 164s 36ms/step - loss: 0.0369 - acc: 0.9885\n",
            "Epoch 9/30\n",
            "4575/4575 [==============================] - 161s 35ms/step - loss: 0.0322 - acc: 0.9898\n",
            "Epoch 10/30\n",
            "4575/4575 [==============================] - 163s 36ms/step - loss: 0.0294 - acc: 0.9911\n",
            "Epoch 11/30\n",
            "4575/4575 [==============================] - 163s 36ms/step - loss: 0.0255 - acc: 0.9919\n",
            "Epoch 12/30\n",
            "4575/4575 [==============================] - 164s 36ms/step - loss: 0.0240 - acc: 0.9926\n",
            "Epoch 13/30\n",
            "4575/4575 [==============================] - 164s 36ms/step - loss: 0.0205 - acc: 0.9934\n",
            "Epoch 14/30\n",
            "4575/4575 [==============================] - 159s 35ms/step - loss: 0.0194 - acc: 0.9937\n",
            "Epoch 15/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0172 - acc: 0.9948\n",
            "Epoch 16/30\n",
            "4575/4575 [==============================] - 155s 34ms/step - loss: 0.0177 - acc: 0.9945\n",
            "Epoch 17/30\n",
            "4575/4575 [==============================] - 155s 34ms/step - loss: 0.0155 - acc: 0.9951\n",
            "Epoch 18/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0151 - acc: 0.9952\n",
            "Epoch 19/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0142 - acc: 0.9955\n",
            "Epoch 20/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0128 - acc: 0.9960\n",
            "Epoch 21/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0123 - acc: 0.9963\n",
            "Epoch 22/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0118 - acc: 0.9962\n",
            "Epoch 23/30\n",
            "4575/4575 [==============================] - 155s 34ms/step - loss: 0.0117 - acc: 0.9964\n",
            "Epoch 24/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0100 - acc: 0.9969\n",
            "Epoch 25/30\n",
            "4575/4575 [==============================] - 155s 34ms/step - loss: 0.0105 - acc: 0.9968\n",
            "Epoch 26/30\n",
            "4575/4575 [==============================] - 155s 34ms/step - loss: 0.0097 - acc: 0.9968\n",
            "Epoch 27/30\n",
            "4575/4575 [==============================] - 155s 34ms/step - loss: 0.0089 - acc: 0.9974\n",
            "Epoch 28/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0090 - acc: 0.9972\n",
            "Epoch 29/30\n",
            "4575/4575 [==============================] - 155s 34ms/step - loss: 0.0083 - acc: 0.9975\n",
            "Epoch 30/30\n",
            "4575/4575 [==============================] - 156s 34ms/step - loss: 0.0081 - acc: 0.9974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd87a87cf98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nej9Qatoc0Gd",
        "colab_type": "code",
        "outputId": "526a14fd-6cdf-419c-8cd0-5a1c9ccdcb4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Evaluación del modelo\n",
        "test_loss, test_acc = model_2.evaluate(np_images_test, labels_categorical_test)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "y_pred = model_2.predict_classes(np_images_test)\n",
        "acc = np.sum(y_pred == np_labels_test) / np.size(np_labels_test)\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2520/2520 [==============================] - 0s 159us/step\n",
            "Test loss: 0.08485885282245777\n",
            "Test accuracy: 0.9809523809523809\n",
            "Test accuracy: 0.9809523809523809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI67G1LZO480",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}