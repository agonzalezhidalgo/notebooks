{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbqHfvwZHnk4"
   },
   "outputs": [],
   "source": [
    "# Instalamos KERAS\n",
    "!pip install -q keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ODuccLWZZxJE",
    "outputId": "202fa509-bd66-4a40-d142-fa23fbf4f47a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dataset_bel'...\n",
      "remote: Enumerating objects: 7345, done.\u001b[K\n",
      "remote: Counting objects: 100% (7345/7345), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7334/7334), done.\u001b[K\n",
      "remote: Total 7345 (delta 2), reused 7342 (delta 2), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (7345/7345), 245.49 MiB | 32.40 MiB/s, done.\n",
      "Resolving deltas: 100% (2/2), done.\n",
      "Checking out files: 100% (7223/7223), done.\n"
     ]
    }
   ],
   "source": [
    "# Descargamos el dataset\n",
    "!git clone https://github.com/agonzalezhidalgo/dataset_bel.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uDKgXudXaYJF",
    "outputId": "caa9aad7-c875-4911-d221-c473681bfebb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-image.org/docs/stable/api/api.html\n",
    "import skimage\n",
    "\n",
    "# https://docs.python.org/3/library/csv.html\n",
    "import csv\n",
    "\n",
    "# https://matplotlib.org/api/index.html\n",
    "import matplotlib\n",
    "\n",
    "# https://matplotlib.org/api/_as_gen/matplotlib.pyplot.html#module-matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://docs.python.org/3/library/os.html\n",
    "import os\n",
    "\n",
    "# https://docs.scipy.org/doc/numpy/reference/\n",
    "import numpy as np\n",
    "\n",
    "# https://keras.io/models/model/\n",
    "import keras\n",
    "\n",
    "from keras import models\n",
    "\n",
    "# Core Layers: https://keras.io/layers/core/\n",
    "# Convolution Layers: https://keras.io/layers/convolutional/\n",
    "from keras import layers\n",
    "\n",
    "# https://keras.io/preprocessing/image/\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# https://keras.io/callbacks/\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16_v-R69B9XO"
   },
   "source": [
    "Celda con funciones generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jP7IhbsUOsc"
   },
   "outputs": [],
   "source": [
    "# Devuelve las imagenes y etiquetas de una carpeta especifíca\n",
    "def __read_images_labels_from_dir(directory, images, labels, shape, as_gray, get_label_from_dir):\n",
    "    \n",
    "    #Buscamos fotos en el directorio\n",
    "    for f in os.listdir(directory):\n",
    "        # Cargamos archivos con extension .ppm \n",
    "        # Obviamos el color y lo cargamos como escala de grises y normalizamos el tamaño\n",
    "        if f.endswith(\".ppm\"):\n",
    "            image = load_image(os.path.join(directory, f), shape, as_gray)\n",
    "            images.append(image)\n",
    "            if get_label_from_dir:\n",
    "                labels.append(int(os.path.basename(directory)))\n",
    "    \n",
    "    return images, labels\n",
    "  \n",
    "# Obtenemos el índice inicial y final de una label específica\n",
    "# - label: índice de la label que se quiere buscar.\n",
    "# - source: lista de la relación de etiquetas.\n",
    "\n",
    "def __get_start_end(label, source):\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    try:        \n",
    "        start = source.index(label)\n",
    "        end = start + source.count(label)\n",
    "\n",
    "    except:\n",
    "        print(\"label doesn't exist\")\n",
    "\n",
    "    return start, end\n",
    "  \n",
    "# Devuelve una colección de directorios de la ruta\n",
    "def __get_directories(data_dir):\n",
    "    \n",
    "    directories = []\n",
    "    \n",
    "    if os.path.exists(data_dir):\n",
    "        \n",
    "        #Buscamos todos los directorios de la ruta\n",
    "        for d in os.listdir(data_dir):\n",
    "            if os.path.isdir(os.path.join(data_dir, d)):\n",
    "                directories.append(d)\n",
    "                \n",
    "    else:\n",
    "        print(\"path doesn't exists\")\n",
    "    return directories\n",
    "  \n",
    "# Devuelve una colección con las imágenes y los labels de la ruta\n",
    "# -datadir: path donde se encuentran la colección de imágenes.\n",
    "# -shape: Dimensiones con las que se cargarán las imágenes.\n",
    "# -as_gray: Indica si la imagen se cargará en escala de grises.\n",
    "# -get_label_from_dir: (defecto True) específica si la categoría se lee del propio directorio\n",
    "def readDataset(data_dir, shape, as_gray, get_label_from_dir = True):\n",
    "    images = []\n",
    "    labels = []\n",
    "    directories = __get_directories(data_dir)\n",
    "    \n",
    "    images, labels = __read_images_labels_from_dir(data_dir, images, labels, shape, as_gray, get_label_from_dir)\n",
    "    \n",
    "    for d in directories:\n",
    "                \n",
    "        #Buscamos fotos en el directorio\n",
    "        images_dir = os.path.join(data_dir, d)\n",
    "        images, labels = __read_images_labels_from_dir(images_dir, images, labels, shape, as_gray, get_label_from_dir)\n",
    "       \n",
    "    return images, labels\n",
    "  \n",
    "# Devuelve una imagen\n",
    "# - path: ruta de la imagen.\n",
    "# - size: dimensión con la que se cargará la imagen.\n",
    "# - as_gray: True para cargar la imagen en escala de grises.\n",
    "\n",
    "def load_image(path, size, as_gray):\n",
    "    aux = skimage.data.imread(path, as_gray = as_gray)\n",
    "    return skimage.transform.resize(aux, size, mode='constant')\n",
    "  \n",
    "# Devuelve una lista con los datos del fichero csv.\n",
    "# - path: ruta hasta el fichero csv.\n",
    "# - delimeter: Carácter delimitador de campos\n",
    "\n",
    "def read_csv(path, delimiter):\n",
    "    file = open(path)\n",
    "    title_csv = csv.reader(file, delimiter = delimiter)\n",
    "    return list(title_csv)\n",
    "  \n",
    "# Imprime los tamaños de las colecciones\n",
    "# - images: lista de imágenes precargadas.\n",
    "# - labels: relación de las imágenes con las categorías a las que pertenecen.\n",
    "# - np_images: np.array con las imágenes precargadas.\n",
    "# - np_labels: np.array con las categorías precargadas.\n",
    "# - environment: string identificativo del entorno.\n",
    "\n",
    "def print_size_dataset(images, labels, np_images, np_labels, environment):\n",
    "    print(\"Total images (\" + environment + \"): \", len(images))\n",
    "    print(\"Total labels (\" + environment + \"): \", len(set(labels)))\n",
    "    print(\"Images shape: \", np_images.shape)\n",
    "    print(\"Labels shape: \", np_labels.shape)\n",
    "    \n",
    "# Imprime una matriz 32x32 con los diferentes tipos de señales que\n",
    "# se van a clasificar. Muestra la primera imagen de cada categoría.\n",
    "# - images: lista de imágenes precargadas.\n",
    "# - labels: relación de las imágenes con las categorías a las que pertenecen.\n",
    "# - titles: lista con los nombres de las categorías.\n",
    "\n",
    "def print_summary_dataset(images, labels, titles):\n",
    "    \n",
    "    #Quitamos los repetidos a los labels\n",
    "    unique_labels = set(labels)\n",
    "    \n",
    "    plt.figure(figsize=(32, 32))\n",
    "    i = 1\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        \n",
    "        #Obtenemos la primera imagén de cada label\n",
    "        image = images[labels.index(label)]\n",
    "        plt.subplot(8, 8, i)\n",
    "        plt.axis('off')\n",
    "        plt.title(titles[i-1][1])\n",
    "        i += 1\n",
    "        _ = plt.imshow(image)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# Imprime todas las imágenes de una label especifica\n",
    "# - label: índice de la label que se quiere buscar.\n",
    "# - images: lista de imágenes precargadas.\n",
    "# - source: lista de la relación de etiquetas.\n",
    "# - titles: lista con las diferentes categorías de labels.\n",
    "\n",
    "def print_signals(label, images, source, titles):\n",
    "    \n",
    "    start, end = __get_start_end(label, source)\n",
    "    \n",
    "    if start < end:\n",
    "        rows = (end - start) / 8\n",
    "\n",
    "        print(\"Signal: \", titles[label][1])\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        i = 1\n",
    "\n",
    "        for image in images[start:end]:\n",
    "            plt.subplot(rows + 1, 8, i)\n",
    "            plt.axis('off')\n",
    "            i += 1\n",
    "            plt.imshow(image)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "# Imprime los atributos de las imágenes de una label especifica.\n",
    "# - label: índice de la label que se quiere buscar.\n",
    "# - images: lista de imágenes precargadas.\n",
    "# - source: lista de la relación de etiquetas.\n",
    "# - titles: lista con las diferentes categorías de labels.\n",
    "\n",
    "def print_signals_attributes(label, images, source, titles):\n",
    "    \n",
    "    start, end = __get_start_end(label, source)\n",
    "    \n",
    "    if start < end:\n",
    "        print(\"Signal: \", titles[label][1])\n",
    "        for image in images[start:end]:\n",
    "            print(\"shape: \", image.shape, \"\\tmin:\", image.min(), \"\\tmax: \", image.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "3WZtFcFKbB6-",
    "outputId": "08fc9e5d-a854-4094-b20e-880235bbbc94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8373415074893782287\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2722368064297962870\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15513400169789513843\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14892338381\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15699205081734854808\n",
      "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Mostramos información de CPU & GPU\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MpZFFb5OZ3Dg",
    "outputId": "6513c77a-a4c0-4686-827c-8970d236666a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de las imágenes de entrada:  (64, 64)\n",
      "Vectorizando la entrada, sería de un tamaño:  4096\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el directorio actual como trabajo.\n",
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "# Establecemos la dimensión de las imágenes.\n",
    "IMG_SHAPE = (64, 64)\n",
    "print(\"Tamaño de las imágenes de entrada: \", IMG_SHAPE)\n",
    "IMG_SHAPE_LEN = IMG_SHAPE[0] * IMG_SHAPE[1]\n",
    "print(\"Vectorizando la entrada, sería de un tamaño: \", IMG_SHAPE_LEN)\n",
    "\n",
    "# Obtenemos los paths de trabajo\n",
    "labels_path = os.path.join(ROOT_PATH, \"dataset_bel/labels.csv\")\n",
    "train_path = os.path.join(ROOT_PATH, \"dataset_bel/train\")\n",
    "test_path = os.path.join(ROOT_PATH, \"dataset_bel/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "n_55nevcUQHR",
    "outputId": "15244a29-1f85-4688-c1cd-907137113ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images (train):  4575\n",
      "Total labels (train):  62\n",
      "Images shape:  (4575, 64, 64, 3)\n",
      "Labels shape:  (4575,)\n",
      "Total images (test):  2520\n",
      "Total labels (test):  53\n",
      "Images shape:  (2520, 64, 64, 3)\n",
      "Labels shape:  (2520,)\n",
      "Titles total:  62\n",
      "Signal:  A31\n",
      "shape:  (64, 64, 3) \tmin: 0.023467419194240183 \tmax:  0.7593352673100491\n",
      "shape:  (64, 64, 3) \tmin: 0.0391180300245094 \tmax:  0.8150242225796563\n",
      "shape:  (64, 64, 3) \tmin: 0.012683823529412207 \tmax:  1.0\n",
      "shape:  (64, 64, 3) \tmin: 0.022579656862743984 \tmax:  0.996078431372549\n",
      "shape:  (64, 64, 3) \tmin: 0.04133013556985346 \tmax:  1.0\n",
      "shape:  (64, 64, 3) \tmin: 0.06338536879595565 \tmax:  0.9201834884344352\n",
      "shape:  (64, 64, 3) \tmin: 0.03416077856924062 \tmax:  0.7854355755974258\n",
      "shape:  (64, 64, 3) \tmin: 0.05793504901960768 \tmax:  0.9921568627450981\n",
      "shape:  (64, 64, 3) \tmin: 0.07595454197303977 \tmax:  0.7563754212622538\n",
      "shape:  (64, 64, 3) \tmin: 0.01371017156862504 \tmax:  0.9921568627450981\n",
      "shape:  (64, 64, 3) \tmin: 0.0064950980392159095 \tmax:  0.996078431372549\n",
      "shape:  (64, 64, 3) \tmin: 0.07279411764705883 \tmax:  0.7749540441176471\n",
      "shape:  (64, 64, 3) \tmin: 0.033517156862744875 \tmax:  0.8648241230085848\n",
      "shape:  (64, 64, 3) \tmin: 0.04844037224264742 \tmax:  0.9921568627450981\n",
      "shape:  (64, 64, 3) \tmin: 0.024230238970588236 \tmax:  0.9842294730392157\n",
      "shape:  (64, 64, 3) \tmin: 0.0081806257659327 \tmax:  0.9777087641697311\n",
      "shape:  (64, 64, 3) \tmin: 0.05310968137254885 \tmax:  0.571966911764706\n",
      "shape:  (64, 64, 3) \tmin: 0.03529411764705882 \tmax:  0.996078431372549\n",
      "shape:  (64, 64, 3) \tmin: 0.03903664981617604 \tmax:  0.9793401979932586\n",
      "shape:  (64, 64, 3) \tmin: 0.0 \tmax:  0.6835477941176472\n",
      "shape:  (64, 64, 3) \tmin: 0.06834022671568622 \tmax:  0.8909658394607853\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las imágenes de entrenamiento y de test.\n",
    "images_train, labels_train = readDataset(train_path, IMG_SHAPE, False)\n",
    "images_test, labels_test = readDataset(test_path, IMG_SHAPE, False)\n",
    "\n",
    "# Convertimos las listas a array numpy de float32\n",
    "np_images_train = np.asarray(images_train, dtype = np.float32)\n",
    "np_labels_train = np.asarray(labels_train, dtype = np.int8)\n",
    "\n",
    "np_images_test = np.asarray(images_test, dtype = np.float32)\n",
    "np_labels_test = np.asarray(labels_test, dtype = np.int8)\n",
    "\n",
    "# Recuperamos los nombres de las categorias. Los diferentes tipo de señales\n",
    "# que se van a clasificar.\n",
    "titles = read_csv(labels_path, \",\")\n",
    "\n",
    "# Se imprime información de los datos cargados.\n",
    "print_size_dataset(images_train, labels_train, np_images_train, np_labels_train, \"train\")\n",
    "print_size_dataset(images_test, labels_test, np_images_test, np_labels_test, \"test\")\n",
    "print(\"Titles total: \", len(titles))\n",
    "print_signals_attributes(10, images_train, labels_train, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2W2tKUgvUukC",
    "outputId": "bd5600b6-3019-4cf2-fd80-d83173c49ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo primera imagen de manera categórica:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Convertimos las labels de manera categórica\n",
    "labels_categorical_train = keras.utils.to_categorical(np_labels_train)\n",
    "labels_categorical_test = keras.utils.to_categorical(np_labels_test)\n",
    "\n",
    "print(\"Ejemplo primera imagen de manera categórica: \", labels_categorical_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhL5rFelZfLv"
   },
   "outputs": [],
   "source": [
    "def get_keras_model():\n",
    "    # IMPLEMENTACIÓN RED NEURONAL\n",
    "    # En Keras la envoltura para cualquier red neuronal se crea con la clase Sequential\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32, (5, 5),\n",
    "                            activation='selu', input_shape=(IMG_SHAPE[0], IMG_SHAPE[1], 3)))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), activation='selu'))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (5, 5), activation='selu'))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(len(set(labels_train)), activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1856
    },
    "colab_type": "code",
    "id": "r8Cucwmicr1K",
    "outputId": "5ab190fb-41da-406e-ef03-7efe55682747"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0615 18:41:02.915286 140185451853696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0615 18:41:02.919599 140185451853696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0615 18:41:02.928430 140185451853696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0615 18:41:02.955195 140185451853696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3217: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0615 18:41:02.961363 140185451853696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0615 18:41:02.965124 140185451853696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0615 18:41:02.973811 140185451853696 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0615 18:41:03.092331 140185451853696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0615 18:41:03.111014 140185451853696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 32)        2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 128)         204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 62)                31806     \n",
      "=================================================================\n",
      "Total params: 290,430\n",
      "Trainable params: 290,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      " - 7s - loss: 2.7702 - acc: 0.3882\n",
      "Epoch 2/25\n",
      " - 1s - loss: 1.1467 - acc: 0.7242\n",
      "Epoch 3/25\n",
      " - 1s - loss: 0.5256 - acc: 0.8658\n",
      "Epoch 4/25\n",
      " - 1s - loss: 0.3055 - acc: 0.9266\n",
      "Epoch 5/25\n",
      " - 1s - loss: 0.2205 - acc: 0.9377\n",
      "Epoch 6/25\n",
      " - 1s - loss: 0.1499 - acc: 0.9604\n",
      "Epoch 7/25\n",
      " - 1s - loss: 0.1218 - acc: 0.9690\n",
      "Epoch 8/25\n",
      " - 1s - loss: 0.1058 - acc: 0.9733\n",
      "Epoch 9/25\n",
      " - 1s - loss: 0.0857 - acc: 0.9773\n",
      "Epoch 10/25\n",
      " - 1s - loss: 0.0667 - acc: 0.9851\n",
      "Epoch 11/25\n",
      " - 1s - loss: 0.0699 - acc: 0.9830\n",
      "Epoch 12/25\n",
      " - 1s - loss: 0.0591 - acc: 0.9836\n",
      "Epoch 13/25\n",
      " - 1s - loss: 0.0394 - acc: 0.9882\n",
      "Epoch 14/25\n",
      " - 1s - loss: 0.0409 - acc: 0.9891\n",
      "Epoch 15/25\n",
      " - 1s - loss: 0.0360 - acc: 0.9897\n",
      "Epoch 16/25\n",
      " - 1s - loss: 0.0263 - acc: 0.9917\n",
      "Epoch 17/25\n",
      " - 1s - loss: 0.0299 - acc: 0.9917\n",
      "Epoch 18/25\n",
      " - 1s - loss: 0.0261 - acc: 0.9937\n",
      "Epoch 19/25\n",
      " - 1s - loss: 0.0236 - acc: 0.9937\n",
      "Epoch 20/25\n",
      " - 1s - loss: 0.0183 - acc: 0.9950\n",
      "Epoch 21/25\n",
      " - 1s - loss: 0.0294 - acc: 0.9923\n",
      "Epoch 22/25\n",
      " - 1s - loss: 0.0167 - acc: 0.9948\n",
      "Epoch 23/25\n",
      " - 1s - loss: 0.0226 - acc: 0.9923\n",
      "Epoch 24/25\n",
      " - 1s - loss: 0.0196 - acc: 0.9937\n",
      "Epoch 25/25\n",
      " - 1s - loss: 0.0240 - acc: 0.9928\n",
      "2520/2520 [==============================] - 0s 166us/step\n",
      "Test loss: 0.2638836495168403\n",
      "Test accuracy: 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "model = get_keras_model()\n",
    "\n",
    "# Muestra la arquitectura de nuestra red neuronal\n",
    "model.summary()\n",
    "\n",
    "# Configurando el modelo de aprendijaze:\n",
    "#  · loss, función para evaluar el grado de error entre salidas calculadas\n",
    "#  · optimizador, función para calcular los pesos de los parámetros a partir de los datos de entrada\n",
    "#  · metricas, para monitorizar el proceso de aprendizaje de la red.\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=\"rmsprop\",\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "# - batch_size, indica el número de datos que se usan en cada actualización.\n",
    "# - epochs, indica el número de veces que se usan todos los datos del proceso.\n",
    "#model.fit(np_images_train, labels_categorical_train, batch_size=32, epochs=20)\n",
    "model.fit(np_images_train, labels_categorical_train,\n",
    "          batch_size = 128,\n",
    "          epochs = 25,\n",
    "          verbose = 2,\n",
    "          callbacks=[ModelCheckpoint('model_64_64_bel_gpu.h5', save_best_only = False)])\n",
    "\n",
    "# Evaluación del modelo\n",
    "test_loss, test_acc = model.evaluate(np_images_test, labels_categorical_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ZgTG5A20csHI",
    "outputId": "e8efb047-734d-4a5c-a096-498387414b50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520/2520 [==============================] - 0s 134us/step\n",
      "Test loss: 0.2638836495168403\n",
      "Test accuracy: 0.9523809523809523\n",
      "Test accuracy: 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "test_loss, test_acc = model.evaluate(np_images_test, labels_categorical_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "y_pred = model.predict_classes(np_images_test)\n",
    "acc = np.sum(y_pred == np_labels_test) / np.size(np_labels_test)\n",
    "print(\"Test accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgI3rrdScsRg"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                             featurewise_std_normalization=False, \n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             zoom_range=0.2,\n",
    "                             shear_range=0.1,\n",
    "                             rotation_range=10.,)\n",
    "\n",
    "datagen.fit(np_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "QftMYJbmcz3d",
    "outputId": "a0883b16-6254-4926-ffce-ab6afdd2cfeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " - 147s - loss: 0.3673 - acc: 0.9120\n",
      "Epoch 2/25\n",
      " - 149s - loss: 0.1964 - acc: 0.9655\n",
      "Epoch 3/25\n",
      " - 149s - loss: 0.2181 - acc: 0.9723\n",
      "Epoch 4/25\n",
      " - 150s - loss: 0.2455 - acc: 0.9759\n",
      "Epoch 5/25\n",
      " - 147s - loss: 0.2772 - acc: 0.9773\n",
      "Epoch 6/25\n",
      " - 147s - loss: 0.2955 - acc: 0.9781\n",
      "Epoch 7/25\n",
      " - 146s - loss: 0.3024 - acc: 0.9785\n",
      "Epoch 8/25\n",
      " - 147s - loss: 0.3091 - acc: 0.9786\n",
      "Epoch 9/25\n",
      " - 147s - loss: 0.2994 - acc: 0.9796\n",
      "Epoch 10/25\n",
      " - 147s - loss: 0.3087 - acc: 0.9795\n",
      "Epoch 11/25\n",
      " - 146s - loss: 0.2851 - acc: 0.9809\n",
      "Epoch 12/25\n",
      " - 147s - loss: 0.2910 - acc: 0.9806\n",
      "Epoch 13/25\n",
      " - 146s - loss: 0.2792 - acc: 0.9815\n",
      "Epoch 14/25\n",
      " - 147s - loss: 0.2816 - acc: 0.9816\n",
      "Epoch 15/25\n",
      " - 147s - loss: 0.2707 - acc: 0.9824\n",
      "Epoch 16/25\n",
      " - 147s - loss: 0.2928 - acc: 0.9810\n",
      "Epoch 17/25\n",
      " - 146s - loss: 0.3029 - acc: 0.9803\n",
      "Epoch 18/25\n",
      " - 146s - loss: 0.2839 - acc: 0.9817\n",
      "Epoch 19/25\n",
      " - 146s - loss: 0.2793 - acc: 0.9820\n",
      "Epoch 20/25\n",
      " - 148s - loss: 0.2913 - acc: 0.9813\n",
      "Epoch 21/25\n",
      " - 148s - loss: 0.2868 - acc: 0.9816\n",
      "Epoch 22/25\n",
      " - 148s - loss: 0.2511 - acc: 0.9838\n",
      "Epoch 23/25\n",
      " - 146s - loss: 0.2763 - acc: 0.9823\n",
      "Epoch 24/25\n",
      " - 146s - loss: 0.2674 - acc: 0.9829\n",
      "Epoch 25/25\n",
      " - 146s - loss: 0.2689 - acc: 0.9827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f1a1abf60>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = get_keras_model()\n",
    "\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=\"rmsprop\",\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model_2.fit_generator(datagen.flow(np_images_train, labels_categorical_train, batch_size=32),\n",
    "                      steps_per_epoch = np_images_train.shape[0],\n",
    "                      epochs = 25,\n",
    "                      verbose = 2,\n",
    "                      callbacks=[ModelCheckpoint('model_data_aug_64_64_bel_gpu.h5',save_best_only = False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "nej9Qatoc0Gd",
    "outputId": "a96a8058-2b80-4c40-b9b3-82f0b0a28480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520/2520 [==============================] - 0s 189us/step\n",
      "Test loss: 0.23908006440080443\n",
      "Test accuracy: 0.9845238095238096\n",
      "Test accuracy: 0.9845238095238096\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "test_loss, test_acc = model_2.evaluate(np_images_test, labels_categorical_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "y_pred = model_2.predict_classes(np_images_test)\n",
    "acc = np.sum(y_pred == np_labels_test) / np.size(np_labels_test)\n",
    "print(\"Test accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RI67G1LZO480"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "traffic_red_convolucional_gpu_bel",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
